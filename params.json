{
  "name": "Latent Dirichlet Allocation (LDA)",
  "tagline": "",
  "body": "# LDA(Latent Dirichlet Allocation)\r\nThe current repository contains the LDA part of the extended abstract on CVPRW'16.\r\nI haven't got the chance to clean up the code. But if you are a beginner in graphical model/probabilistic Bayesian inference, or simply topic modeling, you may check this implementation based on collapsed Gibbs sampling. For details, please check my 10 pages technical report (again just a coarse version) on\r\n http://pages.iu.edu/~zhenk/report.pdf.\r\n\r\n```\r\nAuthor: Kai Zhen\r\nzhenk@umail.iu.edu\r\nwww.zhenk.net\r\nSchool of informatics and computing, Indiana University.\r\n```\r\n\r\n\r\n##How to execute?\r\n\r\n<ul>\r\n<li>Navigate down to the LDACAFFE home directory.</li>\r\n<li>Type \"make\".</li>\r\n<li>Then type ./final 100 \"toyinput.dat\" 4 10 0.1 5 \"file\"</li>\r\n<li>By the way, it means ./final numiteration inputdata numtopic alpha beta numwordspertopic typeofinput</li>\r\n<li>It supports two types of input. The toy one contains all the docs in a single file. Each doc is just a line which can be quite long though. The second one contains all docs in a folder, each doc is a single file. To try the second case, you may want to type ./final 1000 \"/nips12/\" 10 0.5 0.1 5 \"dir\"</li>\r\n<li>By the way, it means ./final numiteration inputdata numtopic alpha beta numwordspertopic typeofinput</li>\r\n</ul>\r\n\r\n##How to check the result?\r\nResults are in theta.txt(distribution of topics for docs), top k words per topic.txt(distribution of words for topics)\r\n\r\n\r\n----------------------------------------------------------------------------------------------------\r\nResult in running all nips papers from 2012\r\n\r\nTopic 0:\r\nlearning\r\nalgorithm\r\ngeneralization\r\ntraining\r\nexamples\r\n\r\nTopic 1:\r\n\r\nIn\r\nmodel\r\nA\r\ndata\r\n\r\nTopic 2:\r\nneurons\r\nneuron\r\nsynaptic\r\ninput\r\nvisual\r\n\r\nTopic 3:\r\nfrequency\r\nsound\r\nstate\r\nsystem\r\ncue\r\n\r\nTopic 4:\r\nMarkov\r\nlanguage\r\nBayesian\r\nsequence\r\nsampling\r\n\r\nTopic 5:\r\nnodes\r\nnetworks\r\nnetwork\r\nnode\r\nvariational\r\n\r\nTopic 6:\r\nimage\r\nimages\r\nfeature\r\nfeatures\r\nmutual\r\n\r\nTopic 7:\r\nstate\r\npolicy\r\nbelief\r\nstates\r\nfunction\r\n\r\nTopic 8:\r\nsources\r\nsource\r\ndata\r\nnonlinear\r\nblind\r\n\r\nTopic 9:\r\nkernel\r\nset\r\nSVM\r\n\r\nsolution\r\n\r\nThe command to run LDA for that result: ./final 1000 \"/nips12/\" 10 0.5 0.1 5 \"dir\"\r\n--------------------------------------------------------------------------------------------------------------\r\nThe result for my toy example that really accompanies me for this month. Thank you toyinput.dat!\r\n\r\nTopic 0:\r\nBeijing\r\nprotests\r\nparty\r\npower\r\n\r\nTopic 1:\r\nlearning\r\nconcepts\r\nobject\r\nhuman\r\n\r\nTopic 2:\r\nman\r\n1989\r\nTiananmen\r\ntank\r\n\r\nTopic 3:\r\nBayesian\r\nmodels\r\ncognitive\r\nintuitive\r\n\r\n\r\n\r\nHow to run and get the above result? ./final 2000 \"toyinput.dat\" 4 0.5 0.1 4 \"di\"\r\nAnd then, sublime top\\ k\\ words\\ per\\ topic.txt\r\n\r\n0.003106 0.208075 0.003106 0.785714 \r\n\r\n0.009804 0.166667 0.009804 0.813725 \r\n\r\n0.015152 0.954545 0.015152 0.015152 \r\n\r\n0.833333 0.038462 0.115385 0.012821 \r\n\r\n0.011905 0.011905 0.940476 0.035714 \r\n\r\n0.006329 0.905063 0.069620 0.018987 \r\n\r\n0.065476 0.017857 0.910714 0.005952 \r\n\r\n0.524510 0.044118 0.426471 0.004902 \r\n\r\n0.801587 0.007937 0.150794 0.039683 \r\n \r\ndoc1\r\n \r\nMy colleagues and I in the Computational Cognitive Science group study one of the most basic and distinctively human aspects of cognition: the ability to learn so much about the world, rapidly and flexibly. Given just a few relevant experiences, even young children can infer the meaning of a new word, the hidden properties of an object or substance, or the existence of a new causal relation or social rule.These inferences go far beyond the data given: after seeing three or four examples of \"horses\", a two-year-old will confidently judge whether any new entity is a horse or not, and she will be mostly correct, except for the occasional donkey or camel. We want to understand these everyday inductive leaps in computational terms. What is the underlying logic that supports reliable generalization from so little data? What are its cognitive and neural mechanisms, and how can we build more powerful learning machines based on the same principles? These questions demand a multidisciplinary approach. Our group's research combines computational models (drawing chiefly on Bayesian statistics, probabilistic generative models, and probabilistic programming) with behavioral experiments in adults and children. Our models make strong quantitative predictions about behavior, but more importantly, they attempt to explain why cognition works, by viewing it as an approximation to ideal statistical inference given the structure of natural tasks and environments. Current research in our group explores the computational basis of many aspects of human cognition: learning concepts, judging similarity, inferring causal connections, forming perceptual representations, learning word meanings and syntactic principles in natural language, noticing coincidences and predicting the future, inferring the mental states of other people, and constructing intuitive theories of core domains, such as intuitive physics, psychology, biology, or social structure.\r\n \r\ndoc2\r\n \r\nWe present an introduction to Bayesian inference as it is used in probabilistic models of cognitive development. Our goal is to provide an intuitive and accessible guide to the what, the how, and the why of the Bayesian approach: what sorts of problems and data the framework is most relevant for, and how and why it may be useful for developmentalists. We emphasize a qualitative understanding of Bayesian inference, but also include information about additional resources for those interested in the cognitive science applications, mathematical foundations, or machine learning details in more depth. In addition, we discuss some important interpretation issues that often arise when evaluating Bayesian models in cognitive science.\r\n\r\ndoc3\r\n\r\nOne-shot learning is an object categorization problem of current research interest in computer vision. Whereas most machine learning based object categorization algorithms require training on hundreds or thousands of images and very large datasets, one-shot learning aims to learn information about object categories from one, or only a few, training images.\r\n\r\ndoc4\r\n\r\nJiang Zemin came to power unexpectedly as a 'compromise candidate' following the Tiananmen Square protests of 1989, when he replaced Zhao Ziyang as General Secretary after Zhao was ousted for his support for the student movement. With the waning influence of Eight Elders due to old age and with the death of Deng Xiaoping, Jiang consolidated his hold on power and became the \"paramount leader\" of the country in the 1990s.\r\n\r\ndoc5\r\n\r\nA man who stood in front of a column of tanks on June 5, 1989, the morning after the Chinese military had suppressed the Tiananmen Square protests of 1989 by force, became known as the Tank Man, Unknown Protester or Unknown Rebel. As the lead tank maneuvered to pass by the man, he repeatedly shifted his position in order to obstruct the tank's attempted path around him. The incident was filmed and seen worldwide.\r\n\r\ndoc6\r\n\r\nPeople learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.\r\n\r\ndoc7\r\n\r\nThe incident took place near Tiananmen on Chang'an Avenue, which runs east-west along the north end of Tiananmen Square in Beijing, China, on June 5, 1989, one day after the Chinese government's violent crackdown on the Tiananmen protests.[4] The man stood in the middle of the wide avenue, directly in the path of a column of approaching Type 59 tanks. He wore a white shirt and black trousers, and he held a shopping bag.[5] As the tanks came to a stop, the man gestured towards the tanks with his bag. In response, the lead tank attempted to drive around the man, but the man repeatedly stepped into the path of the tank in a show of nonviolent action.[6] After repeatedly attempting to go around rather than crush the man, the lead tank stopped its engines, and the armored vehicles behind it seemed to follow suit. There was a short pause with the man and the tanks having reached a quiet, still impasse.\r\n\r\ndoc8\r\n\r\nDuring the 1989 student demonstrations in Beijing, the Chinese People's Liberation Army (PLA) played a decisive role in enforcing martial law, suppressing the demonstrations by force and upholding the authority of the Chinese Communist Party. The scale of the military's mobilization for a domestic event and degree of bloodshed inflicted against civilians were unprecedented both in the history of the People's Republic and the history of Beijing, a city with a tradition of popular protests against ruling authorities dating back to the May Fourth Movement of 1919. The subject of the Tiananmen protests in general and the military's role in the crackdown remains forbidden from public discussion in China.[4] The killings in Beijing continue to taint the legacies of the party elders, led by Deng Xiaoping, and weigh on the generation of leaders whose careers advanced as their more moderate colleagues were purged or sidelined at the time.[4] Within China, the role of the military in 1989 remains a subject of private discussion within the ranks of the party leadership and PLA.[4] Only outside of China is the subject part of the public discourse.\r\n\r\ndoc9\r\n\r\nThe protests were triggered in April 1989 by the death of former Communist Party General Secretary Hu Yaobang, a liberal reformer who was deposed after losing a power struggle with hardliners over the direction of political and economic reforms.[8] University students marched and gathered in Tiananmen Square to mourn. Hu had also voiced grievances against inflation, limited career prospects, and corruption of the party elite.[9] The protesters called for government accountability, freedom of the press, freedom of speech, and the restoration of workers' control over industry.[10][11] At the height of the protests, about a million people assembled in the Square.[12] Most of them were university students in Beijing.\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}